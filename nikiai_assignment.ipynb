{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intent Classification\n",
    "\n",
    "### Preprocessing:\n",
    "    1. Remove \"'s\" and non alphabets.\n",
    "    2. Convert from capital to lower.\n",
    "    2. Stem using SnowBallStemmer.\n",
    "    \n",
    "### Feature Extraction\n",
    "    1. CountVectorizer using top 50 features\n",
    "\n",
    "### Model Selection and Evaluation:\n",
    "    1. Cross Validation CV with 5 folds. Tried with SVM, MultinomialNB, MLPClassifier, DecisionTreeClassifier, and \n",
    "       RandomForestClassifier\n",
    "    2. Use Precision, Recall, FScore\n",
    "\n",
    "### Hyperparameter Optimization:\n",
    "    1. RandomizedSearchCV (Because Grid Search is too costly)\n",
    "\n",
    "### Result:\n",
    "__The best model trained was 98.5% accurate__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import (train_test_split,\n",
    "                                    cross_val_score)\n",
    "from sklearn.metrics import (confusion_matrix, \n",
    "                             precision_recall_fscore_support, \n",
    "                             f1_score, accuracy_score, recall_score,\n",
    "                             precision_score)\n",
    "\n",
    "from sklearn.metrics import pre\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Label                                               Text\n",
      "0  unknown  how did serfdom develop in and then leave russ...\n",
      "1     what   what films featured the character popeye doyle ?\n",
      "2  unknown  how can i find a list of celebrities ' real na...\n",
      "3     what  what fowl grabs the spotlight after the chines...\n",
      "4     what                    what is the full form of .com ?\n"
     ]
    }
   ],
   "source": [
    "with open(\"train.txt\") as f:\n",
    "    text = f.read()\n",
    "text = text.split(\"\\n\")\n",
    "corpus = []\n",
    "labels = []\n",
    "for datum in text:\n",
    "    text_line = datum.split(\",,,\")\n",
    "    if len(text_line) != 2:\n",
    "        continue\n",
    "    corpus.append(text_line[0].strip())\n",
    "    labels.append(text_line[1].strip())\n",
    "corpus_dict = {\"Text\": corpus, \"Label\": labels}\n",
    "df = pd.DataFrame(corpus_dict)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['unknown', 'what', 'when', 'who', 'affirmation'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Label\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def stem_filter(row):\n",
    "    row = re.sub(r\"'s'\", \"\", row)\n",
    "    row = re.sub(\"[^a-zA-Z ]\", \"\", row)\n",
    "    words = word_tokenize(row)\n",
    "    stemmed = []\n",
    "    for word in words:\n",
    "        stemmed.append(stemmer.stem(word))\n",
    "    filtered_line = \" \".join(stemmed[:4])\n",
    "    return filtered_line\n",
    "\n",
    "df[\"Text\"] = df[\"Text\"].apply(stem_filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(max_features=50)\n",
    "X = cv.fit_transform(df[\"Text\"])\n",
    "y = df[\"Label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.97993311  0.97651007  0.97297297  0.94594595  0.98979592]\n"
     ]
    }
   ],
   "source": [
    "clf = MLPClassifier()\n",
    "print(cross_val_score(clf, X, y, cv=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization\n",
    "\n",
    "WARNING: Do not execute this cell for it will take a long time to complete. This model was trained on Google Colaboratory. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    'learning_rate': [\"constant\", \"invscaling\", \"adaptive\"],\n",
    "    'hidden_layer_sizes': [(100, 100), (200, 200), (300, 300)],\n",
    "    'alpha': np.linspace(0.001, 0.1, 10),\n",
    "    'activation': [\"logistic\", \"relu\", \"tanh\"]\n",
    "}\n",
    "\n",
    "clf = RandomizedSearchCV(estimator=MLPClassifier(),\n",
    "                         param_distributions=parameters, cv=7, verbose=2, \n",
    "                         n_jobs=-1, n_iter=50)\n",
    "clf.fit(X, y)\n",
    "print(\"Best Model: {}\".format(clf.best_estimator_))\n",
    "print(\"Best Score: {}\".format(clf.best_score_))\n",
    "print(\"Best Params: {}\".format(clf.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Best Model: MLPClassifier(activation='relu', alpha=0.1, batch_size='auto', beta_1=0.9,\n",
    "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
    "       hidden_layer_sizes=(100, 100), learning_rate='invscaling',\n",
    "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
    "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
    "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
    "       verbose=False, warm_start=False)\n",
    "Best Score: 0.9850505731625084\n",
    "Best Params: {'learning_rate': 'invscaling', 'hidden_layer_sizes': (100, 100), 'alpha': 0.1, 'activation': 'relu'}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.1, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100, 100), learning_rate='invscaling',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.2,\n",
    "                                                    stratify=y)\n",
    "model = MLPClassifier(learning_rate=\"invscaling\", \n",
    "                      hidden_layer_sizes=(100, 100), \n",
    "                      alpha=0.1, activation=\"relu\")\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fscore  of Individual class\n",
      "affirmation    0.952381\n",
      "unknown        0.962264\n",
      "what           0.983607\n",
      "when           0.950000\n",
      "who            1.000000\n",
      "dtype: float64\n",
      "Accuracy :  0.979797979798\n",
      "Recall :  0.979797979798\n",
      "Precision :  0.980309813643\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted affirmation</th>\n",
       "      <th>Predicted unknown</th>\n",
       "      <th>Predicted what</th>\n",
       "      <th>Predicted when</th>\n",
       "      <th>Predicted who</th>\n",
       "      <th>Actual Total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>affirmation</th>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unknown</th>\n",
       "      <td>0</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>what</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>when</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>who</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>81</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Predicted Total</th>\n",
       "      <td>21</td>\n",
       "      <td>52</td>\n",
       "      <td>122</td>\n",
       "      <td>21</td>\n",
       "      <td>81</td>\n",
       "      <td>297</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Predicted affirmation  Predicted unknown  Predicted what  \\\n",
       "affirmation                         20                  1               0   \n",
       "unknown                              0                 51               2   \n",
       "what                                 1                  0             120   \n",
       "when                                 0                  0               0   \n",
       "who                                  0                  0               0   \n",
       "Predicted Total                     21                 52             122   \n",
       "\n",
       "                 Predicted when  Predicted who  Actual Total  \n",
       "affirmation                   0              0            21  \n",
       "unknown                       1              0            54  \n",
       "what                          1              0           122  \n",
       "when                         19              0            19  \n",
       "who                           0             81            81  \n",
       "Predicted Total              21             81           297  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = model.classes_\n",
    "predictions = model.predict(X_test)\n",
    "f1score = list(f1_score(y_test, predictions, average=None))\n",
    "fscore = pd.Series(f1score, index=labels)\n",
    "print(\"Fscore  of Individual class:\")\n",
    "print(fscore)\n",
    "print(\"Accuracy : {}\".format(accuracy_score(y_test, predictions)))\n",
    "print(\"Recall : {}\".format(recall_score(y_test,predictions, average=\"weighted\")))\n",
    "print(\"Precision : {}\".format(precision_score(y_test,predictions, average=\"weighted\")))\n",
    "# confusion matrix\n",
    "pred_labels = ['Predicted '+ l for l in labels]\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "cm = pd.DataFrame(cm, index=labels, columns=pred_labels)\n",
    "cm['Actual Total'] = cm.sum(axis=1)\n",
    "cm.loc['Predicted Total'] = cm.sum()\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating final model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.1, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100, 100), learning_rate='invscaling',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model = MLPClassifier(learning_rate=\"invscaling\", \n",
    "                            hidden_layer_sizes=(100, 100), \n",
    "                            alpha=0.1, activation=\"relu\")\n",
    "final_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentence(sentence):\n",
    "    filtered = stem_filter(sentence)\n",
    "    vector = cv.transform([filtered])\n",
    "    probabilities = final_model.predict_proba(vector)\n",
    "    classes = final_model.classes_\n",
    "    y_pred = np.max(probabilities)\n",
    "    if y_pred < 0.6:\n",
    "        pred = \"unknown\"\n",
    "    else:\n",
    "        pred = classes[np.argmax(probabilities)]\n",
    "    print(\"Predicted class: \\\"{0}\\\" with confidence {1:.2f} %\"\n",
    "              .format(pred, y_pred * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with your own sentence\n",
    "Substitute your sentence in the `input_sentence` field below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: \"affirmation\" with confidence 98.47 %\n"
     ]
    }
   ],
   "source": [
    "# Input your sentence below\n",
    "input_sentence = \"Will I be hired? :) \"\n",
    "predict_sentence(input_sentence)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
